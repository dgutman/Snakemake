# Felipe Giuste
# 5/17/2019
# Snakefile
# Randomise Whole Brain Nifti Analysis

import glob, os

#-------------------------------------------------------------------------------
# Constants
#-------------------------------------------------------------------------------
nifti_directory= "/mnt/gv0/FoxNiftis/"
randomise_directory= "/mnt/gv0/randomise/"
model_directory = "/mnt/gv0/model/"
design_matrix = "%s/group_comparison.mat" % model_directory
contrast_matrix = "%s/group_comparison.con" % model_directory
npermutations = "5000"
#ncontrasts = np.genfromtxt(contrasts, skip_header=4, comments='/').shape[0]


#-------------------------------------------------------------------------------
# Config
#-------------------------------------------------------------------------------
# SAMPLES: Folder name containing niftis for a row (128 Sources):
SAMPLES = [fname.split('/')[-1] for fname in glob.glob('%s/*'%nifti_directory)]
RANDOMISE = expand(randomise_directory+"{sample}_done", sample = SAMPLES)

#-------------------------------------------------------------------------------
# Functions
#-------------------------------------------------------------------------------
from collections import Counter
import glob, os, csv
import numpy as np

# Function grabs data matrix within nifti file (no header)
def getNII(nii):
    import nibabel as nib
    import numpy as np
    image_nii = nib.nifti1.load(nii)
    data_nii = image_nii.get_fdata(dtype=np.float64)
    image_nii.uncache()
    return( data_nii )

#-------------------------------------------------------------------------------
# Rules
#-------------------------------------------------------------------------------
rule all:
    input: RANDOMISE[300]
    shell:
        """
        echo "Finito";
        """

# Execute Randomise on each nifti within a RowSlice folder: Starts Container=neuroimaging_{wildcards.sample}
rule randomise:
    input: niftidir= nifti_directory+"{sample}"
    output: randomise_directory+"{sample}_done"
    shell:
        """
        docker run -t -d --rm --name neuroimaging_{wildcards.sample} fgiuste/neuroimaging
        mkdir -p {randomise_directory}/{wildcards.sample};
        for nii in `ls {input[niftidir]}`
        do
            run_randomise="randomise -i {input[niftidir]}/$nii -o {randomise_directory}/{wildcards.sample}/$nii/$nii -d {design_matrix} -t {contrast_matrix} -n {npermutations} -R -N -x --permout";
            docker exec neuroimaging_{wildcards.sample} echo $run_randomise > {randomise_directory}/{wildcards.sample}/$nii;
        done
        docker stop neuroimaging_{wildcards.sample}
        mv {randomise_directory}/{wildcards.sample} {output}
        """

# rule permutation:
#     input: randomise_directory+"{sample}_done"
#     output: permutation_directory+"{sample}_done"
#     run:
#         # Iterate through contrasts:
#         ncontrasts=3
#         for contrast in range(1, ncontrasts+1):
#             #print('### Contrast: %s ###' % contrast)
#             perms= glob.glob('%s/*_vox_tstat%s_perm*.nii.gz' % ({output}, contrast) )
#             perms.sort(key= lambda x: int(x.split('_perm')[-1].split('.nii.gz')[0]) )

#             counterF= Counter()
#             for i in perms:
#                 # load nifti data:
#                 tmp= getNII(i)
#                 # round to tenths:
#                 tmp= np.round(tmp, decimals=1)
#                 # save as counter:
#                 countertmp= Counter(tmp.flatten())
#                 # Add to final counter:
#                 counterF= counterF + countertmp
#                 # delete perm data
#                 os.remove(i)

#             # Save counterF to {output} directory as 'NullTCounter.csv';
#             with open('%s/NullTCounter_%s.csv' % ({output}, contrast),'w') as csvfile:
#                 writer=csv.writer(csvfile, delimiter='\t')
#                 for key, value in counterF.items():
#                     writer.writerow([key,value])

# TODO: create slurm job config file to specify ntasks=1,c=1, manually override for larger jobs
## Also: slurm.conf: CR_CPU -> CR_CPU_Memory

# snakemake --jobs 500 --cluster "sbatch --ntasks=1 --cpus-per-task=2"
